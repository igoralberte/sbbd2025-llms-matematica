# LLMs São Bons Matemáticos? Avaliando o Desempenho em Resolução de Exercícios - Materiais suplementares

***Resumo:*** Este trabalho analisa o **uso de dois modelos de LLMs comerciais (Google Gemini 2.5 Pro e OpenAI ChatGPT 4o) para resolução de exercícios de Matemática de Nível Médio sobre cinco diferentes tópicos que incluem Funções, Geometria, Análise Combinatória e outros**. Ao todo, foram resolvidas 50 **questões elaboradas pela banca FGV (Fundação Getúlio Vargas)**. Ambos os modelos tiveram resultados semelhantes, com o Gemini sendo ligeiramente melhor, tanto no raciocínio quanto na escolha da alternativa correta. Nas questões em que o ChatGPT apresentou erro, sua versão o3-pro foi capaz de acertá-las. **Os resultados podem subsidiar decisões de pessoas e organizações envolvidas com ensino-aprendizagem de Matemática sobre o uso da tecnologia.**

**O trabalho vinculado a este repositório será apresentado no SBBD/LAGO 2025 em Setembro.**

## Materiais suplementares
Neste repositório, estão disponíveis as questões utilizadas para a avaliação das LLMs e suas variações sem alternativas. 

Você encontrará:
- A questão utilizada
- A resposta dada pelo ChatGPT 4o e pelo o3-pro, quando aplicável (em caso de erro do 4o)
- A resposta dada pelo Gemini 2.5 Pro

Também está disponível a análise do tamanho das respostas dadas por cada modelo.

